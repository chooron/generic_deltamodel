{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy model to evaluate efficiencies of multi-GPU methods\n",
    "23 Jan. 2025\n",
    "\n",
    "---\n",
    "\n",
    "#### Toy Model (ReLU)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import time\n",
    "\n",
    "from core.utils import set_randomseed\n",
    "set_randomseed()\n",
    "\n",
    "\n",
    "# Define a simple toy model\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Check if GPUs are available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train vanilla...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vanilla(dataloader, main_device, epochs, input_size, output_size, hidden_size, lr):\n",
    "    print(\"\\n=== Single-GPU Training ===\")\n",
    "    model = ToyModel(input_size, hidden_size, output_size).to(main_device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    t0 = time.time()\n",
    "    t_epoch = 0.0  # avg time per epoch\n",
    "    t_back = 0.0   # avg time per backward\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        t_ep0 = time.time()\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move data to main device\n",
    "            inputs, labels = inputs.to(main_device), labels.to(main_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            t_back0 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_back1 = time.time()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            t_back += t_back1 - t_back0\n",
    "        \n",
    "        t_ep1 = time.time()\n",
    "        t_epoch += t_ep1 - t_ep0\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # Total runtime\n",
    "    t1 = time.time()\n",
    "    t_total = t1 - t0\n",
    "\n",
    "    # Average time per epoch\n",
    "    t_epoch /= epochs\n",
    "\n",
    "    # Average time per backward\n",
    "    t_back /= epochs * len(dataloader)\n",
    "\n",
    "    print(f\"------ \\n Training: {t_total:.4f} seconds.\")\n",
    "    print(f\"------ \\n Average time per epoch: {t_epoch:.4f} seconds.\")\n",
    "    print(f\"------ \\n Average time per backward: {t_back:.4f} seconds.\")\n",
    "    print(\"====================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train with DataParallel...\n",
    "\n",
    "- Splits the input (*batch*) across multiple GPUs\n",
    "    - Module is replicated on each device to handle a piece of the input\n",
    "- Executes forward/backward pass on each GPU independently\n",
    "- Aggregate grads on primary GPU before updating params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dp(dataloader, main_device, device_ids, epochs, input_size, output_size, hidden_size, lr):\n",
    "    print(\"\\n=== DataParallel Training ===\")\n",
    "    model = ToyModel(input_size, hidden_size, output_size).to(main_device)\n",
    "\n",
    "    # Wrap the model with DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {len(device_ids)} GPUs with DataParallel.\")\n",
    "        model = nn.DataParallel(\n",
    "            model,\n",
    "            output_device=main_device,\n",
    "            device_ids=device_ids\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"DataParallel requires more than 1 GPU.\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    t0 = time.time()\n",
    "    t_epoch = 0.0  # avg time per epoch\n",
    "    t_back = 0.0   # avg time per backward\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        t_ep0 = time.time()\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move data to main device\n",
    "            inputs, labels = inputs.to(main_device), labels.to(main_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            t_back0 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_back1 = time.time()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            t_back += t_back1 - t_back0\n",
    "        \n",
    "        t_ep1 = time.time()\n",
    "        t_epoch += t_ep1 - t_ep0\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # Total runtime\n",
    "    t1 = time.time()\n",
    "    t_total = t1 - t0\n",
    "\n",
    "    # Average time per epoch\n",
    "    t_epoch /= epochs\n",
    "\n",
    "    # Average time per backward\n",
    "    t_back /= epochs * len(dataloader)\n",
    "\n",
    "    print(f\"------ \\n Training: {t_total:.4f} seconds.\")\n",
    "    print(f\"------ \\n Average time per epoch: {t_epoch:.4f} seconds.\")\n",
    "    print(f\"------ \\n Average time per backward: {t_back:.4f} seconds.\")\n",
    "    print(\"====================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train with DistributedDataParallel..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DistributedDataParallel Training ===\n",
      "[rank0]:[W125 13:25:25.008844384 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "W0125 13:25:25.826000 3572048 torch/multiprocessing/spawn.py:160] Terminating process 3572223 via signal SIGTERM\n",
      "Traceback (most recent call last):\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/deltaModel/train_ddp.py\", line 172, in <module>\n",
      "    mp.spawn(\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 328, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 284, in start_processes\n",
      "    while not context.join():\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 203, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/deltaModel/train_ddp.py\", line 96, in train_ddp\n",
      "    setup(rank, world_size)\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/deltaModel/train_ddp.py\", line 70, in setup\n",
      "    torch.cuda.set_device(gpus[rank])  # Set the device for each rank\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python train_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare training variations\n",
    "\n",
    "---\n",
    "\n",
    "#### Prepare synthetic data and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Dataset parameters\n",
    "input_size = 1000\n",
    "output_size = 2\n",
    "num_samples = 10000\n",
    "batch_size = 10000\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "device_ids = [6,7]\n",
    "main_device = 6\n",
    "epochs = 2\n",
    "hidden_size = 256\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "### Synthetic dataset\n",
    "# Random input features and labels\n",
    "x = torch.rand(num_samples, input_size)\n",
    "y = torch.randint(0, output_size, (num_samples,))\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run training variants..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single-GPU Training ===\n",
      "Epoch [1/2], Loss: 0.6999\n",
      "Epoch [2/2], Loss: 1.3098\n",
      "------ \n",
      " Training: 0.2042 seconds.\n",
      "------ \n",
      " Average time per epoch: 0.1020 seconds.\n",
      "------ \n",
      " Average time per backward: 0.0014 seconds.\n",
      "====================================\n",
      "\n",
      "\n",
      "=== DataParallel Training ===\n",
      "Using 2 GPUs with DataParallel.\n",
      "Epoch [1/2], Loss: 0.6996\n",
      "Epoch [2/2], Loss: 1.3484\n",
      "------ \n",
      " Training: 0.2063 seconds.\n",
      "------ \n",
      " Average time per epoch: 0.1031 seconds.\n",
      "------ \n",
      " Average time per backward: 0.0031 seconds.\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_vanilla(dataloader, main_device, epochs, input_size, output_size, hidden_size, lr)\n",
    "\n",
    "train_dp(dataloader, main_device, device_ids, epochs, input_size, output_size, hidden_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
