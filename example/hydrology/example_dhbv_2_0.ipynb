{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: **$\\delta$ HBV 2.0**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to forward a pre-trained $\\delta$ HBV 2.0UH model developed by [Yalan Song et al. (2024)](https://doi.org/10.22541/essoar.172736277.74497104/v1). For explanation of model structure, methodologies, [data](https://mhpi.github.io/datasets/CONUS/#results), and performance metrics, please refer to Song's publication [below](#publication). If you find this code is useful in your own work, please include the aforementioned citation.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Before Running:\n",
    "- **Environment**: From `env/` a minimal Python environment can be setup for running this code... (see `docs/getting_started.md` for more details.)\n",
    "    - Conda -- `deltamodel_env.yaml`\n",
    "    - Pip -- `requirements.txt`\n",
    "\n",
    "\n",
    "- **Model and Data**: The trained $\\delta$ HBV 2.0 model and input data can be downloaded from [sharepoint](https://pennstateoffice365-my.sharepoint.com/:f:/g/personal/cxs1024_psu_edu/Eqi1NuJ3d2pMpEJpVu0EGSoBigi-VCWVHgOYIRoTeuGiOw?e=HaNNeA). After downloading...\n",
    "\n",
    "    1. Update the `subbasin_data_path` key in data config `example/conf/observations/merit_forward.yaml` with your path to `MERIT_input_sample/71_0`.\n",
    "\n",
    "    2. Update the `trained_model` key in model config `example/conf/config_dhbv_2_0.yaml` with the path to you directory containing the trained model `dHBV_2_0_Ep100.pt` AND normalization file `statics_basinnorm.json`.\n",
    "\n",
    "- **Hardware**: The LSTMs used in this model require CUDA support only available with Nvidia GPUs. For those without access, T4 GPUs can be used when running this notebook with dMG on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Publication:\n",
    "\n",
    "*Song, Yalan, Tadd Bindas, Chaopeng Shen, Haoyu Ji, Wouter Johannes Maria Knoben, Leo Lonzarich, Martyn P. Clark et al. \"High-resolution national-scale water modeling is enhanced by multiscale differentiable physics-informed machine learning.\" Authorea Preprints (2024). https://essopenarchive.org/doi/full/10.22541/essoar.172736277.74497104.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Issues:\n",
    "For questions, concerns, bugs, etc., please reach out by posting an issue on the [dMG repo](https://github.com/mhpi/generic_deltaModel/issues).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Train/Evaluate $\\delta$ HBV 2.0\n",
    "\n",
    "*Multiscale training for dHBV2.0 is not currently enabled in dMG. Training code will be released at a later time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward $\\delta$ HBV 2.0\n",
    "\n",
    "After completing [these](#before-running) steps, forward the $\\delta$ HBV 2.0 model with the code block below.\n",
    "\n",
    "--> For default settings expect evaluation time of ~2 minutes with an Nvidia A100.\n",
    "\n",
    "**Note**\n",
    "- The settings defined in `../example/conf/config_dhbv_2_0.yaml` are set to replecate benchmark performance.\n",
    "- For model evaluation, set `mode: predict` in `example/conf/config_dhbv_2_0.yaml`, or modify after the config dict has been created (see below).\n",
    "- The default inference window is set from 1 January 1980 to 31 December 2020, which should use ~70GB of vram.\n",
    "- The first year (`warm_up` in the config, 365 days is default) of the inference period is used for initializing HBV's internal states (water storages) and is, therefore, excluded from the model's prediction output.\n",
    "- If you are new to the *dMG* framework and want further explanation and exposure of the methods used below, we suggest first looking at our notebook for $\\delta$ HBV 1.0: `example/hydrology/example_dhbv_1_0.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCurrent Configuration\u001b[0m\n",
      "  Experiment Mode:    test                \n",
      "  Model 1:            HBV_2_0             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data Source:        merit_forward       \n",
      "  Test Range :        1980/01/01          2020/12/31          \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Train Epochs:       100                 Batch Size:         100                 \n",
      "  LSTM Dropout:       0.5                 LSTM Hidden Size:   64                  \n",
      "  MLP Dropout:        0.5                 MLP Hidden Size:    4096                \n",
      "  Warmup:             0                   Concurrent Models:  4                   \n",
      "  Loss Fn:            RmseLossComb        \n",
      "\n",
      "\u001b[1mMachine\u001b[0m\n",
      "  Use Device:         cuda:0              \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/8 [00:00<?, ?it/s]/projects/mhpi/lglonz/project_silmaril/generic_deltaModel/example/hydrology/../../deltaModel/models/neural_networks/lstm_models.py:104: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the dMG root directory.\n",
    "\n",
    "from example import load_config \n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "from core.utils import print_config\n",
    "from core.utils.module_loaders import get_data_loader, get_trainer\n",
    "from trainers.trainer_ms import Trainer\n",
    "from core.data.data_loaders.loader_hydro_ms import get_dataset_dict\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_2_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "print_config(config)\n",
    "\n",
    "# 2. Setup a dataset dictionary of NN and physics model inputs.\n",
    "# data_loader = get_data_loader(config['data_loader'])\n",
    "# data_loader = data_loader(config, test_split=True, overwrite=False)\n",
    "inference_dataset = get_dataset_dict(config, train=False)\n",
    "\n",
    "# 3. Initialize the differentiable model dHBV 2.0 (LSTM + HBV 2.0).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 4. Initialize trainer to handle forward pass.\n",
    "trainer = get_trainer(config['trainer'])\n",
    "trainer = trainer(\n",
    "    config,\n",
    "    model,\n",
    "    inf_dataset=inference_dataset,\n",
    "    eval_dataset= inference_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Forward pass through the model to get streamflow predictions.\n",
    "predictions = trainer.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Predictions\n",
    "\n",
    "After running model inference we can, e.g., view the hydrograph for one of the basins to see we are getting expected outputs.\n",
    "\n",
    "We can do this with our target variable, streamflow, for instance... (though, there are many other states and fluxes we can output as shown in the output cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from core.utils.dates import Dates\n",
    "from core.data import txt_to_array\n",
    "from core.post.plot_hydrograph import plot_hydrograph\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose a basin by USGS gage ID to plot.\n",
    "GAGE_ID = 1022500\n",
    "TARGET = 'flow_sim'\n",
    "\n",
    "# Resample to 3-day prediction. Options: 'D', 'W', 'M', 'Y'.\n",
    "RESAMPLE = '3D'\n",
    "\n",
    "# Set the paths to the gage ID lists...\n",
    "GAGE_ID_PATH = 'your/path/to/gage_ids.npy'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "print(f\"HBV states and fluxes: {predictions['HBV_2_0'].keys()} \\n\")\n",
    "\n",
    "\n",
    "# 1. Get the streamflow predictions and daily timesteps of the prediction window.\n",
    "streamflow_pred = predictions['HBV_2_0'][TARGET]\n",
    "timesteps = Dates(config['predict'], config['dpl_model']['rho']).batch_daily_time_range\n",
    "\n",
    "# Remove warm-up period to match model output (see Note above.)\n",
    "timesteps = timesteps[config['dpl_model']['phy_model']['warm_up']:]\n",
    "\n",
    "\n",
    "# 2. Load the gage ID list and get the basin index.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "print(f\"First 20 available gage IDs: \\n {gage_ids[:20]} \\n\")\n",
    "\n",
    "if GAGE_ID in gage_ids:\n",
    "    basin_idx = list(gage_ids).index(GAGE_ID)\n",
    "else:\n",
    "    raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the MERIT dataset.\")\n",
    "\n",
    "# 3. Get the data for the chosen basin and plot.\n",
    "streamflow_pred_basin = streamflow_pred[:, basin_idx].squeeze()\n",
    "\n",
    "plot_hydrograph(\n",
    "    timesteps,\n",
    "    streamflow_pred_basin,\n",
    "    streamflow_pred_basin,\n",
    "    resample=RESAMPLE,\n",
    "    title=f\"Hydrograph for Gage ID {GAGE_ID}\",\n",
    "    ylabel='Streamflow (ft$^3$/s)',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
