{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHPI hydroDL2.0 Tutorial: **dHBV1.1p**\n",
    "---\n",
    "\n",
    "This is a basic implementation of the generic differentiable modeling framework `dMG` using the HBV1.1p hydrology model\n",
    "plugin from the `hydroDL2.0` repository.\n",
    "\n",
    "\n",
    "\n",
    "Last Revision: 30 Oct. 2024\n",
    "\n",
    "Authors; Leo Lonzarich\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Basic Hands-off Deployment:\n",
    "\n",
    "In this first demonstration, we show how `dMG` using a HBV1.1p physics model backbone from `hydroDL2` can be operated\n",
    "in a  few steps. These are outlined as follows:\n",
    "\n",
    "0. First, ensure that you have the correct *env* configured. To avoid manually downloading required Python packages,\n",
    "create a `hydrodl` env using \n",
    "\n",
    "    `conda env create -f envs/hydrodl_env.yaml`.\n",
    "\n",
    "    Once activated, confirm PyTorch installed correctly with `torch.cuda.is_available()`. If this reports false, try\n",
    "    - `conda uninstall pytorch`\n",
    "    - `conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n",
    "\n",
    "1. Set your desired model and experiment configuration settings within a *yaml* config file.\n",
    "    - For this tutorial, you can find this config located at `generic_diffModel/example/conf/dhbv_11p_config.yaml`. Note,\n",
    "    this yaml is configured to reproduce dHBV1.1p benchmarks for 531 CAMELS basins, trained and tested for 9 and 10 years,\n",
    "    respectively\n",
    "    - For normal operation of `dMG`, however, see `generic_diffModel/conf/config.yaml`.\n",
    "2. Either run `python dMG/__main__.py` in your terminal, or (recommended) run the contents of `__main__.py` in the cells below.\n",
    "    - This will parse your config into a dictionary, load the HBV1.1p hydrology model, and begin training or testing.\n",
    "\n",
    "\n",
    "### 1.1 Create Configurations Dictionary\n",
    "\n",
    "The first cell below will convert the configurations yaml file into a key-indexed dictionary, with keys being the\n",
    "config settings. \n",
    "\n",
    "That is, if `mode: train` is set in `dhbv_11p_config.yaml`, the dictionary will yield `config['mode'] == 'train'`.\n",
    "Similarly, `training: start_time: 1999/10/01` is equivalent to `config['training']['start_time'] == '1999/10/01'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dMG configuration file with dHBV1.1p options:\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "\n",
    "\n",
    "# Example configs stored in /example/conf\n",
    "CONFIG_PATH = '../conf'\n",
    "CONFIG_NAME = 'dhbv_11p_config'\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path: str, config_name: str) -> DictConfig:\n",
    "    \"\"\" Initialize Hydra and parse model configuration yaml(s) into config dict. \"\"\"\n",
    "    with hydra.initialize(config_path=config_path, version_base='1.3'):\n",
    "        config = hydra.compose(config_name=config_name)\n",
    "   \n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "    return config_dict\n",
    "\n",
    "config = load_config(CONFIG_PATH, CONFIG_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run `__main__.py` with Configurations\n",
    "\n",
    "This code instantiates a model Trainer which will train or test a model per the user's specification in the config. Note\n",
    "that `__main__.py` is trimmed-down here to illustrate it's primary objective.\n",
    "Within the Trainer itself, \n",
    "- CAMELS data will be loaded and preprocessed,\n",
    "- A differenial model object with the HBV1.1p backbone will be created, and \n",
    "- An optimizer and loss function will be initialized.\n",
    "\n",
    "These and other details/structure of `dMG` will be illustrated in the second part of this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mCurrent Configuration\u001b[0m\n",
      "  Experiment Mode:    train               \n",
      "  Ensemble Mode:      none                \n",
      "  Model 1:            HBV                 \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data Source:        camels_531          \n",
      "  Train Range :       1999/10/01          2008/10/01          \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Train Epochs:       50                  Batch Size:         100                 \n",
      "  Dropout:            0.5                 Hidden Size:        256                 \n",
      "  Warmup:             365                 Concurrent Models:  16                  \n",
      "  Optimizer:          RmseLossFlowComb    \n",
      "\n",
      "\u001b[1mMachine\u001b[0m\n",
      "  Use Device:         cuda                \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/194 [00:00<?, ?it/s]/data/lgl5139/project_blue_eyes/generic_diffModel/example/dPLHBV/../../dMG/models/neural_networks/lstm_models.py:103: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647348947/work/aten/src/ATen/native/cudnn/RNN.cpp:1410.)\n",
      "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
      "Epoch 1/50:   1%|          | 1/194 [00:03<12:01,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.5342836380004883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   1%|          | 2/194 [00:05<07:36,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.2687909603118896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   2%|▏         | 3/194 [00:06<06:14,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.173997402191162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   2%|▏         | 4/194 [00:08<05:37,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.303285598754883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   3%|▎         | 5/194 [00:09<05:17,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.1490955352783203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   3%|▎         | 6/194 [00:11<05:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.1225171089172363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   4%|▎         | 7/194 [00:12<04:57,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.154280662536621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   4%|▍         | 8/194 [00:14<04:56,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.9577683210372925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   5%|▍         | 9/194 [00:15<04:58,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.7685747146606445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   5%|▌         | 10/194 [00:17<04:49,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.9212005138397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   6%|▌         | 11/194 [00:19<04:53,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  2.0355677604675293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   6%|▌         | 12/194 [00:20<04:52,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.5010920763015747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   7%|▋         | 13/194 [00:22<04:48,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.6582947969436646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   7%|▋         | 14/194 [00:24<05:01,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.8150919675827026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   8%|▊         | 15/194 [00:25<04:54,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss:  1.9237264394760132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     run_train_test(config)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(config_dict)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Run an experiment based on the mode specified in the configuration. \"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m experiment_handler \u001b[38;5;241m=\u001b[39m build_handler(config_dict)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mexperiment_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/project_blue_eyes/generic_diffModel/example/dPLHBV/../../dMG/trainers/train.py:54\u001b[0m, in \u001b[0;36mTrainModel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_ep, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     52\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngrid_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch_stats(epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_loss_dict, minibatch_iter, start_time)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/data/lgl5139/project_blue_eyes/generic_diffModel/example/dPLHBV/../../dMG/trainers/train.py:81\u001b[0m, in \u001b[0;36mTrainModel._train_epoch\u001b[0;34m(self, epoch, minibatch_iter, ngrid_train, nt, optim)\u001b[0m\n\u001b[1;32m     78\u001b[0m hydro_loss, ep_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdplh_model_handler\u001b[38;5;241m.\u001b[39mcalc_loss(ep_loss_dict)\n\u001b[1;32m     80\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m hydro_loss\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/hydrodl/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/hydrodl/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/.conda/envs/hydrodl/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../dMG') # Add the root directory of dMG to the path\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "from conf.config import ModeEnum\n",
    "from trainers import build_handler\n",
    "from core.utils import (create_output_dirs, set_randomseed, set_system_spec,\n",
    "                        print_config)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def run_train_test(config_dict: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Run training and testing as one experiment.\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    config_dict['mode'] = ModeEnum.train\n",
    "    train_experiment_handler = build_handler(config_dict)\n",
    "    train_experiment_handler.run()\n",
    "\n",
    "    # Testing\n",
    "    config_dict['mode'] = ModeEnum.test\n",
    "    test_experiment_handler = build_handler(config_dict)            \n",
    "    test_experiment_handler.dplh_model_handler = train_experiment_handler.dplh_model_handler\n",
    "    test_experiment_handler.run()\n",
    "\n",
    "\n",
    "def run_experiment(config_dict: Dict[str, Any]) -> None:\n",
    "    \"\"\" Run an experiment based on the mode specified in the configuration. \"\"\"\n",
    "    experiment_handler = build_handler(config_dict)\n",
    "    experiment_handler.run()\n",
    "\n",
    "\n",
    "\n",
    "# Set device, dtype, output directories, and random seed.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "config['device'], config['dtype'] = set_system_spec(config['gpu_id'])\n",
    "config = create_output_dirs(config)\n",
    "\n",
    "log.info(f\"RUNNING MODE: {config['mode']}\")\n",
    "print_config(config)\n",
    "\n",
    "# Run training and testing together, or one at a time.\n",
    "if config['mode'] == ModeEnum.train_test:\n",
    "    run_train_test(config)\n",
    "\n",
    "else:\n",
    "    run_experiment(config)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get Results of Tested Model\n",
    "\n",
    "If you have run testing on a trained model and want to view the results, you can find a `mstd.csv` file in your model\n",
    "directory, which will give you the statistics on your model's performance. \n",
    "\n",
    "*Graphical visualizations of model output will be supported in a future updated.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Breakdown of Intermediate Steps: Training\n",
    "\n",
    "In this example, we break down dHBV1.1p differentiable model training in `dMG` by exposing the internals of the Trainer.\n",
    "(**Note**, we are bypassing `__main__.py` in this part since it simply runs the Trainer.)\n",
    "\n",
    "### 2.1 Create Configurations Dictionary\n",
    "\n",
    "Once again, begin by creating a configurations dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dMG configuration file with dHBV1.1p options:\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import hydra\n",
    "\n",
    "\n",
    "# Example configs stored in /example/conf\n",
    "CONFIG_PATH = '../conf'\n",
    "CONFIG_NAME = 'dhbv_11p_config'\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path: str, config_name: str) -> DictConfig:\n",
    "    \"\"\" Initialize Hydra and parse model configuration yaml(s) into config dict. \"\"\"\n",
    "    with hydra.initialize(config_path=config_path, version_base='1.3'):\n",
    "        config = hydra.compose(config_name=config_name)\n",
    "   \n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "    return config_dict\n",
    "\n",
    "config = load_config(CONFIG_PATH, CONFIG_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize model, optimizer and loss function\n",
    "\n",
    "These are the auxillary tasks first completed by the Trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
