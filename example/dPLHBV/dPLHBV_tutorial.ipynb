{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHPI hydroDL2.0 Tutorial: **dHBV1.1p**\n",
    "---\n",
    "\n",
    "This is a basic implementation of the generic differentiable modeling framework `dMG` using the HBV1.1p hydrology model\n",
    "plugin from the `hydroDL2.0` repository.\n",
    "\n",
    "\n",
    "\n",
    "Last Revision: 30 Oct. 2024\n",
    "\n",
    "Authors: Leo Lonzarich\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Basic Hands-off Deployment:\n",
    "\n",
    "In this first demonstration, we show how `dMG` using a HBV1.0 or HBV1.1p physics model backbone from `hydroDL2` can be operated\n",
    "in a  few steps. These are outlined as follows:\n",
    "\n",
    "0. First, ensure that you have the correct *env* configured. To avoid manually downloading required Python packages,\n",
    "create a `hydrodl` env using \n",
    "\n",
    "    `conda env create -f envs/hydrodl_env.yaml`.\n",
    "\n",
    "    Once activated, confirm PyTorch installed correctly with `torch.cuda.is_available()`. If this reports false, try\n",
    "    - `conda uninstall pytorch`\n",
    "    - `conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n",
    "\n",
    "1. Set your desired model and experiment configuration settings within a *yaml* config file.\n",
    "    - For this tutorial, you can find this config located at `generic_diffModel/example/conf/dhbv_11p_config.yaml`. Note,\n",
    "    this yaml is configured to reproduce dHBV1.1p benchmarks for 531 CAMELS basins, trained and tested for 9 and 10 years,\n",
    "    respectively\n",
    "    - For normal operation of `dMG`, however, see `generic_diffModel/conf/config.yaml`.\n",
    "2. Either run `python dMG/__main__.py` in your terminal, or (recommended) run the contents of `__main__.py` in the cells below.\n",
    "    - This will parse your config into a dictionary, load the HBV1.1p hydrology model, and begin training or testing.\n",
    "\n",
    "\n",
    "### 1.1 Create Configurations Dictionary\n",
    "\n",
    "- `dhbv_config.yaml` for HBV1.0\n",
    "- `dhbv_11p_config.yaml` for HBV1.1p\n",
    "\n",
    "The first cell below will convert the configurations yaml file into a key-indexed dictionary, with keys being the\n",
    "config settings. \n",
    "\n",
    "That is, if `mode: train` is set in the config file, the dictionary will yield `config['mode'] == 'train'`.\n",
    "Similarly, `training: start_time: 1999/10/01` is equivalent to `config['training']['start_time'] == '1999/10/01'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dMG configuration file with dHBV1.1p options:\n",
    "import sys\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "sys.path.append('../../dMG') # Add the root directory of dMG to the path\n",
    "\n",
    "\n",
    "# Example configs stored in /example/conf\n",
    "CONFIG_PATH = '../conf'\n",
    "CONFIG_NAME = 'dhbv_11p_config'\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path: str, config_name: str) -> DictConfig:\n",
    "    \"\"\" Initialize Hydra and parse model configuration yaml(s) into config dict. \"\"\"\n",
    "    with hydra.initialize(config_path=config_path, version_base='1.3'):\n",
    "        config = hydra.compose(config_name=config_name)\n",
    "   \n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "    return config_dict\n",
    "\n",
    "config = load_config(CONFIG_PATH, CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run `__main__.py` with Configurations\n",
    "\n",
    "This code instantiates a model Trainer which will train or test a model per the user's specification in the config. Note\n",
    "that `__main__.py` is trimmed-down here to illustrate it's primary objective. The trainer performs the following\n",
    "functions:\n",
    "- CAMELS data will be loaded and preprocessed,\n",
    "- A differenial model object with the HBV1.1p backbone will be created, and \n",
    "- An optimizer and loss function will be initialized.\n",
    "\n",
    "These and other details/structure of `dMG` will be illustrated in the second part of this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "from conf.config import ModeEnum\n",
    "from trainers import build_handler\n",
    "from core.utils import (create_output_dirs, set_randomseed, set_system_spec,\n",
    "                        print_config)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def run_train_test(config_dict: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Run training and testing as one experiment.\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    config_dict['mode'] = ModeEnum.train\n",
    "    train_experiment_handler = build_handler(config_dict)\n",
    "    train_experiment_handler.run()\n",
    "\n",
    "    # Testing\n",
    "    config_dict['mode'] = ModeEnum.test\n",
    "    test_experiment_handler = build_handler(config_dict)            \n",
    "    test_experiment_handler.dplh_model_handler = train_experiment_handler.dplh_model_handler\n",
    "    test_experiment_handler.run()\n",
    "\n",
    "\n",
    "def run_experiment(config_dict: Dict[str, Any]) -> None:\n",
    "    \"\"\" Run an experiment based on the mode specified in the configuration. \"\"\"\n",
    "    experiment_handler = build_handler(config_dict)\n",
    "    experiment_handler.run()\n",
    "\n",
    "\n",
    "\n",
    "# Set device, dtype, output directories, and random seed.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "config['device'], config['dtype'] = set_system_spec(config['gpu_id'])\n",
    "config = create_output_dirs(config)\n",
    "\n",
    "log.info(f\"RUNNING MODE: {config['mode']}\")\n",
    "print_config(config)\n",
    "\n",
    "# Run training and testing together, or one at a time.\n",
    "if config['mode'] == ModeEnum.train_test:\n",
    "    run_train_test(config)\n",
    "\n",
    "else:\n",
    "    run_experiment(config)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get Results of Tested Model\n",
    "\n",
    "If you have run testing on a trained model and want to view the results, you can find a `mstd.csv` file in your model\n",
    "directory, which will give you the statistics on your model's performance. \n",
    "\n",
    "*Graphical visualizations of model output will be supported in a future updated.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Breakdown of Intermediate Steps: Training\n",
    "\n",
    "In this example, we break down dHBV1.1p differentiable model training in `dMG` by exposing the internals of the Trainer.\n",
    "(**Note**, we are bypassing `__main__.py` in this part since it simply runs the Trainer.)\n",
    "\n",
    "### 2.1 Create Configurations Dictionary\n",
    "\n",
    "Once again, begin by creating a configurations dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dMG configuration file with dHBV1.1p options:\n",
    "import sys\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "sys.path.append('../../dMG') # Add the root directory of dMG to the path\n",
    "\n",
    "from core.utils import create_output_dirs, set_system_spec\n",
    "\n",
    "\n",
    "\n",
    "# Example configs stored in /example/conf\n",
    "CONFIG_PATH = '../conf'\n",
    "CONFIG_NAME = 'dhbv_11p_config'\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path: str, config_name: str) -> DictConfig:\n",
    "    \"\"\" Initialize Hydra and parse model configuration yaml(s) into config dict. \"\"\"\n",
    "    with hydra.initialize(config_path=config_path, version_base='1.3'):\n",
    "        config = hydra.compose(config_name=config_name)\n",
    "   \n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "\n",
    "    config_dict['device'], config_dict['dtype'] = set_system_spec(config_dict['gpu_id'])\n",
    "    config_dict = create_output_dirs(config_dict)\n",
    "\n",
    "    return config_dict\n",
    "\n",
    "config = load_config(CONFIG_PATH, CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the Data\n",
    "\n",
    "In this tutorial, we work with either 671 or 531 CAMELS basins.\n",
    "\n",
    "MHPI Team\n",
    "- For immediate access to the CAMELS train/test data files, run this tutorial on Suntzu server. Data paths are already\n",
    "preconfigured for this server so nothing further needs to be done. See path specs here: \n",
    "`generic_diffModel/dMG/conf/observations/camels_531.yaml`.\n",
    "\n",
    "Else\n",
    "- Expanded instruction will be added for obtaining this data once `hydro_data` and `dMG` reach v1.0 release.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.dataset_loading import get_data_dict\n",
    "\n",
    "dataset_dict, config = get_data_dict(config, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialize model, optimizer and loss function\n",
    "\n",
    "These are the auxillary tasks completed by the Trainer before beginning the training loop.\n",
    "\n",
    "\n",
    "#### 2.3.1 Init Differentiable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from models.neural_networks.lstm_models import CudnnLstmModel\n",
    "from models.neural_networks.mlp_models import MLPmul\n",
    "from hydroDL2 import load_model\n",
    "\n",
    "model_name = config['phy_model']['models'][0]\n",
    "\n",
    "\n",
    "\n",
    "def init_phy_model(config, model_name):\n",
    "    \"\"\"Initialize the physics model.\"\"\"\n",
    "    if model_name == 'HBV':\n",
    "        hydro_model = load_model('HBV')\n",
    "        return hydro_model(config)\n",
    "    elif model_name == 'HBV_11p':\n",
    "        hydro_model = load_model('HBV_11p')\n",
    "        return hydro_model()\n",
    "    else:\n",
    "        raise ValueError(model_name, \"is not a valid physics model.\")\n",
    "\n",
    "\n",
    "def init_pnn_model(config, phy_model):\n",
    "    \"\"\"Initialize the pNN model.\"\"\"\n",
    "    n_forc = len(config['observations']['nn_forcings'])\n",
    "    n_attr = len(config['observations']['nn_attributes'])\n",
    "    n_model_params = len(phy_model.parameters_bound)\n",
    "    n_rout_params = len(phy_model.conv_routing_hydro_model_bound)\n",
    "    \n",
    "    nx = n_forc + n_attr\n",
    "    ny = config['dpl_model']['nmul'] * n_model_params\n",
    "\n",
    "    if config['routing_hydro_model'] == True:\n",
    "        ny += n_rout_params\n",
    "    \n",
    "    if config['pnn_model']['model'] == 'LSTM':\n",
    "        nn_model = CudnnLstmModel(\n",
    "            nx=nx,\n",
    "            ny=ny,\n",
    "            hiddenSize=config['pnn_model']['hidden_size'],\n",
    "            dr=config['pnn_model']['dropout']\n",
    "        )\n",
    "    elif config['pnn_model']['model'] == 'MLP':\n",
    "        nn_model = MLPmul(\n",
    "            config,\n",
    "            nx=nx,\n",
    "            ny=ny\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(config['pnn_model'], \" not supported.\")\n",
    "    \n",
    "    return nn_model\n",
    "    \n",
    "\n",
    "\n",
    "# 1. Initialize the physics model\n",
    "phy_model = init_phy_model(config, model_name)\n",
    "\n",
    "# 2. Initialize the parameterization neural network (pNN) model.\n",
    "pnn_model = init_pnn_model(config, phy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for extracting the physics model parameters from the pNN output. (Not necessary to expand)\n",
    "def breakdown_params(config, phy_model, params_all):\n",
    "    \"\"\"Extract physics model parameters from pNN output.\"\"\"\n",
    "    n_model_params = len(phy_model.parameters_bound)\n",
    "    n_rout_params = len(phy_model.conv_routing_hydro_model_bound)\n",
    "\n",
    "    ny = config['dpl_model']['nmul'] * n_model_params\n",
    "    if config['routing_hydro_model'] == True:\n",
    "        ny += n_rout_params\n",
    "\n",
    "    params_dict = dict()\n",
    "    params_hydro_model = params_all[:, :, :ny]\n",
    "\n",
    "    # Physics model params.\n",
    "    params_dict['hydro_params_raw'] = torch.sigmoid(\n",
    "        params_hydro_model[:, :, :len(phy_model.parameters_bound) * config['dpl_model']['nmul']]).view(\n",
    "        params_hydro_model.shape[0], params_hydro_model.shape[1], len(phy_model.parameters_bound),\n",
    "        config['dpl_model']['nmul'])\n",
    "    \n",
    "    # Routing params\n",
    "    if config['routing_hydro_model'] == True:\n",
    "        params_dict['conv_params_hydro'] = torch.sigmoid(\n",
    "            params_hydro_model[-1, :, len(phy_model.parameters_bound) * config['dpl_model']['nmul']:])\n",
    "    else:\n",
    "        params_dict['conv_params_hydro'] = None\n",
    "    return params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the differentiable model object and a basic forward method.\n",
    "class dHBV(torch.nn.Module):\n",
    "    def __init__(self, phy_model, pnn_model):\n",
    "        super(dHBV, self).__init__()\n",
    "        self.pnn_model = pnn_model\n",
    "        self.phy_model = phy_model\n",
    "\n",
    "    def forward(self, config, x_dict):\n",
    "        \"\"\"Basic Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_dict : dict\n",
    "            Dictionary containing the input data for the model.\n",
    "        \"\"\"\n",
    "        # Forward pNN model;\n",
    "        # Take array of predicted params from pNN and unpack into dictionary.\n",
    "        params_all = self.pnn_model(x_dict['inputs_nn_scaled'])\n",
    "        params_dict = breakdown_params(config, self.phy_model, params_all)\n",
    "        \n",
    "        # Forward physics model\n",
    "        flow_out = self.phy_model(\n",
    "            x_dict['x_hydro_model'],\n",
    "            params_dict['hydro_params_raw'],\n",
    "            config,\n",
    "            static_idx=config['phy_model']['stat_param_idx'],\n",
    "            warm_up=config['phy_model']['warm_up'],\n",
    "            routing=config['routing_hydro_model'],\n",
    "            conv_params_hydro=params_dict['conv_params_hydro']\n",
    "        )\n",
    "\n",
    "        # Baseflow index percentage;\n",
    "        # Using two deep groundwater buckets: gwflow & bas_shallow\n",
    "        if 'bas_shallow' in flow_out.keys():\n",
    "            baseflow = flow_out['gwflow'] + flow_out['bas_shallow']\n",
    "        else:\n",
    "            baseflow = flow_out['gwflow']\n",
    "        flow_out['BFI_sim'] = 100 * (torch.sum(baseflow, dim=0) / (\n",
    "                torch.sum(flow_out['flow_sim'], dim=0) + 0.00001))[:, 0]\n",
    "\n",
    "        return flow_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize the differentiable model object, and assign to a GPU.\n",
    "dpl_model = dHBV(phy_model, pnn_model).to(config['device'])\n",
    "print(dpl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Init Optimizer\n",
    "\n",
    "NOTE: currently a workaround, but will give a descriptive perscription in future updates.\n",
    "\n",
    "The model handler object contains an optimizer and loss function that can be used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_handler import ModelHandler\n",
    "\n",
    "model_handler = ModelHandler(config)\n",
    "\n",
    "# Pass the dpl_model into the model_handler.\n",
    "model_handler.model_dict = {model_name: dpl_model}\n",
    "\n",
    "\n",
    "# Get the learnable pNN parameters from the dpl_model for the optimizer.\n",
    "model_params = dpl_model.parameters()\n",
    "model_handler.all_model_params = model_params\n",
    "\n",
    "\n",
    "# Initialize the optimizer (torch Adadelta).\n",
    "model_handler.init_optimizer()\n",
    "optim = model_handler.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Init Loss Function\n",
    "\n",
    "NOTE: currently a workaround with model handler, but will updated shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function (named in your configuration file).\n",
    "model_handler.init_loss_func(dataset_dict['obs'])\n",
    "loss_func = model_handler.loss_func\n",
    "\n",
    "# See that the corerct loss function is loaded.\n",
    "print(f\"Config setting: {config['loss_function']['model']}\")\n",
    "print(f\"Loaded loss function: {model_handler.loss_func}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from core.data import n_iter_nt_ngrid, take_sample_train\n",
    "from core.utils import save_model\n",
    "\n",
    "\n",
    "\n",
    "# Setup training grid.\n",
    "ngrid_train, minibatch_iter, nt = n_iter_nt_ngrid(\n",
    "    dataset_dict['inputs_nn_scaled'], config['train_t_range'], config\n",
    "    )\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    ep_loss_dict = {model_name: 0.0}  # Initialize loss to zero.\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "    \n",
    "    for i in tqdm.tqdm(range(1, minibatch_iter + 1), desc=prog_str,\n",
    "                        leave=False, dynamic_ncols=True):\n",
    "        \n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_dict_sample = take_sample_train(config, dataset_dict,\n",
    "                                                ngrid_train, nt)\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        model_preds = dpl_model.forward(config, dataset_dict_sample)        \n",
    "        \n",
    "        # Calculate loss.\n",
    "        model_handler.flow_out_dict = {model_name: model_preds}\n",
    "        model_handler.dataset_dict_sample = dataset_dict_sample\n",
    "\n",
    "        hydro_loss, ep_loss_dict = model_handler.calc_loss(ep_loss_dict)\n",
    "        \n",
    "        total_loss = hydro_loss\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save the model every save_epoch (set in the confic).\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
