{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHPI hydroDL2.0 Tutorial: **dHBV1.1p**\n",
    "---\n",
    "\n",
    "This is a basic implementation of the generic differentiable modeling framework `dMG` using the HBV1.1p hydrology model\n",
    "plugin from the `hydroDL2.0` repository.\n",
    "\n",
    "\n",
    "\n",
    "Last Revision: 30 Oct. 2024\n",
    "\n",
    "Authors: Leo Lonzarich\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Basic Hands-off Deployment:\n",
    "\n",
    "In this first demonstration, we show how `dMG` using a HBV1.0 or HBV1.1p physics model backbone from `hydroDL2` can be operated\n",
    "in a  few steps. These are outlined as follows:\n",
    "\n",
    "0. First, ensure that you have the correct *env* configured. To avoid manually downloading required Python packages,\n",
    "create a `hydrodl` env using \n",
    "\n",
    "    `conda env create -f envs/hydrodl_env.yaml`.\n",
    "\n",
    "    Once activated, confirm PyTorch installed correctly with `torch.cuda.is_available()`. If this reports false, try\n",
    "    - `conda uninstall pytorch`\n",
    "    - `conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n",
    "\n",
    "1. Set your desired model and experiment configuration settings within a *yaml* config file.\n",
    "    - For this tutorial, you can find this config located at `generic_diffModel/example/conf/dhbv_v1.1p_config.yaml`. Note,\n",
    "    this yaml is configured to reproduce dHBV1.1p benchmarks for 531 CAMELS basins, trained and tested for 9 and 10 years,\n",
    "    respectively\n",
    "    - For normal operation of `dMG`, however, see `generic_diffModel/conf/config.yaml`.\n",
    "2. Either run `python dMG/__main__.py` in your terminal, or (recommended) run the contents of `__main__.py` in the cells below.\n",
    "    - This will parse your config into a dictionary, load the HBV1.1p hydrology model, and begin training or testing.\n",
    "\n",
    "\n",
    "### 1.1 Create Configurations Dictionary\n",
    "\n",
    "- `dhbv_config.yaml` for HBV1.0\n",
    "- `dhbv_v1.1p_config.yaml` for HBV1.1p\n",
    "\n",
    "The first cell below will convert the configurations yaml file into a key-indexed dictionary, with keys being the\n",
    "config settings. \n",
    "\n",
    "That is, if `mode: train` is set in the config file, the dictionary will yield `config['mode'] == 'train'`.\n",
    "Similarly, `training: start_time: 1999/10/01` is equivalent to `config['training']['start_time'] == '1999/10/01'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dMG configuration file with dHBV1.1p options:\n",
    "import sys\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "sys.path.append('../../deltaMod') # Add the root directory of dMG to the path\n",
    "from core.utils import initialize_config\n",
    "\n",
    "# Example configs stored in /example/conf\n",
    "CONFIG_PATH = '../conf'\n",
    "CONFIG_NAME = 'dhbv_v1_1p_config'\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config: str, config_name: str) -> DictConfig:\n",
    "    \"\"\" Initialize Hydra and parse model configuration yaml(s) into config dict. \"\"\"\n",
    "    with hydra.initialize(config_path=config, version_base='1.3'):\n",
    "        config = hydra.compose(config_name=config_name)\n",
    "   \n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "\n",
    "    initialize_config(config_dict)\n",
    "    return config_dict\n",
    "\n",
    "config = load_config(CONFIG_PATH, CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run `__main__.py` with Configurations\n",
    "\n",
    "This code instantiates a model Trainer which will train or test a model per the user's specification in the config. Note\n",
    "that `__main__.py` is trimmed-down here to illustrate it's primary objective. The trainer performs the following\n",
    "functions:\n",
    "- CAMELS data will be loaded and preprocessed,\n",
    "- A differenial model object with the HBV1.1p backbone will be created, and \n",
    "- An optimizer and loss function will be initialized.\n",
    "\n",
    "These and other details/structure of `dMG` will be illustrated in the second part of this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from conf.config import ModeEnum\n",
    "from trainers import build_handler\n",
    "from models.model_handler import ModelHandler as dModel\n",
    "\n",
    "from core.utils import (create_output_dirs, set_randomseed, set_system_spec,\n",
    "                        print_config)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def run_train_test(config, model):\n",
    "    \"\"\"\n",
    "    Run training and testing as one experiment.\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    config['mode'] = ModeEnum.train\n",
    "    train_experiment_handler = build_handler(config, model)\n",
    "    train_experiment_handler.run()\n",
    "\n",
    "    # Testing\n",
    "    config['mode'] = ModeEnum.test\n",
    "    test_experiment_handler = build_handler(config, model)            \n",
    "    test_experiment_handler.model = train_experiment_handler.model # Use the trained model\n",
    "    test_experiment_handler.run()\n",
    "\n",
    "\n",
    "def run_experiment(config, model):\n",
    "    \"\"\" Run an experiment based on the mode specified in the configuration. \"\"\"\n",
    "    experiment_handler = build_handler(config, model)\n",
    "    experiment_handler.run()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a model\n",
    "model = dModel(config).to(config['device'])\n",
    "\n",
    "# Set device, dtype, output directories, and random seed.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "config['device'], config['dtype'] = set_system_spec(config['gpu_id'])\n",
    "config = create_output_dirs(config)\n",
    "\n",
    "log.info(f\"RUNNING MODE: {config['mode']}\")\n",
    "print_config(config)\n",
    "\n",
    "# Run training and testing together, or one at a time.\n",
    "if config['mode'] == ModeEnum.train_test:\n",
    "    run_train_test(config)\n",
    "\n",
    "else:\n",
    "    run_experiment(config, model)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get Results of Tested Model\n",
    "\n",
    "If you have run testing on a trained model and want to view the results, you can find a `mstd.csv` file in your model\n",
    "directory, which will give you the statistics on your model's performance. \n",
    "\n",
    "*Graphical visualizations of model output will be supported in a future updated.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Breakdown of Intermediate Steps: Training\n",
    "\n",
    "In this example, we break down dHBV1.1p differentiable model training in `dMG` by exposing the internals of the Trainer.\n",
    "(**Note**, we are bypassing `__main__.py` in this part since it simply runs the Trainer.)\n",
    "\n",
    "### 2.1 Create Configurations Dictionary\n",
    "\n",
    "Once again, begin by creating a configurations dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dMG configuration file with dHBV1.1p options:\n",
    "import sys\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "sys.path.append('../../deltaMod') # Add the root directory of dMG to the path\n",
    "from core.utils import initialize_config\n",
    "\n",
    "\n",
    "\n",
    "# Example configs stored in /example/conf\n",
    "CONFIG_PATH = '../conf'\n",
    "CONFIG_NAME = 'dhbv_v1_1p_config'\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config: str, config_name: str) -> DictConfig:\n",
    "    \"\"\" Initialize Hydra and parse model configuration yaml(s) into config dict. \"\"\"\n",
    "    with hydra.initialize(config_path=config, version_base='1.3'):\n",
    "        config = hydra.compose(config_name=config_name)\n",
    "   \n",
    "    config_dict = OmegaConf.to_container(config, resolve=True)\n",
    "\n",
    "    initialize_config(config_dict)\n",
    "    return config_dict\n",
    "\n",
    "config = load_config(CONFIG_PATH, CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the Data\n",
    "\n",
    "In this tutorial, we work with either 671 or 531 CAMELS basins.\n",
    "\n",
    "MHPI Team\n",
    "- For immediate access to the CAMELS train/test data files, run this tutorial on Suntzu server. Data paths are already\n",
    "preconfigured for this server so nothing further needs to be done. See path specs here: \n",
    "`generic_diffModel/dMG/conf/observations/camels_531.yaml`.\n",
    "\n",
    "Else\n",
    "- Expanded instruction will be added for obtaining this data once `hydro_data` and `dMG` reach v1.0 release.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.dataset_loading import get_dataset_dict\n",
    "\n",
    "dataset = get_dataset_dict(config, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialize model, optimizer and loss function\n",
    "\n",
    "These are the auxillary tasks completed by the Trainer before beginning the training loop.\n",
    "\n",
    "\n",
    "#### 2.3.1 Init Differentiable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "from models.neural_networks.lstm_models import CudnnLstmModel\n",
    "from models.neural_networks.mlp_models import MLPmul\n",
    "from hydroDL2 import load_model\n",
    "\n",
    "\n",
    "\n",
    "def init_nn_model(phy_model, config):\n",
    "    \"\"\"Initialize the pNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phy_model : torch.nn.Module\n",
    "        The physics model.\n",
    "    config : dict\n",
    "        The configuration dictionary.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The initialized neural network.\n",
    "    \"\"\"\n",
    "    n_forc = len(config['nn_forcings'])\n",
    "    n_attr = len(config['nn_attributes'])\n",
    "    n_model_params = len(phy_model.parameters_bound)\n",
    "    n_rout_params = len(phy_model.conv_routing_hydro_model_bound)\n",
    "    \n",
    "    nx = n_forc + n_attr\n",
    "    ny = config['dpl_model']['nmul'] * n_model_params\n",
    "\n",
    "    if config['routing_hydro_model'] == True:\n",
    "        ny += n_rout_params\n",
    "    \n",
    "    if config['pnn_model']['model'] == 'LSTM':\n",
    "        nn_model = CudnnLstmModel(\n",
    "            nx=nx,\n",
    "            ny=ny,\n",
    "            hiddenSize=config['pnn_model']['hidden_size'],\n",
    "            dr=config['pnn_model']['dropout']\n",
    "        )\n",
    "    elif config['pnn_model']['model'] == 'MLP':\n",
    "        nn_model = MLPmul(\n",
    "            config,\n",
    "            nx=nx,\n",
    "            ny=ny\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(config['pnn_model'], \" not supported.\")\n",
    "    \n",
    "    return nn_model\n",
    "    \n",
    "\n",
    "\n",
    "# 1. Initialize the physics model either with import or load_model.\n",
    "# from hydroDL2.models.hbv import hbv, hbv_v1_1p\n",
    "model_name = config['phy_model']['model'][0]\n",
    "phy_model = load_model(model_name)\n",
    "phy_model = phy_model().to(config['device'])\n",
    "\n",
    "# 2. Initialize the parameterization neural network (pNN) model.\n",
    "pnn_model = init_pnn_model(config, phy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for extracting the physics model parameters from the pNN output. (Not necessary to expand)\n",
    "def breakdown_params(config, phy_model, params_all):\n",
    "    \"\"\"Extract physics model parameters from pNN output.\"\"\"\n",
    "    n_model_params = len(phy_model.parameters_bound)\n",
    "    n_rout_params = len(phy_model.conv_routing_hydro_model_bound)\n",
    "\n",
    "    ny = config['dpl_model']['nmul'] * n_model_params\n",
    "    if config['routing_hydro_model'] == True:\n",
    "        ny += n_rout_params\n",
    "\n",
    "    params_dict = dict()\n",
    "    params_hydro_model = params_all[:, :, :ny]\n",
    "\n",
    "    # Physics model params.\n",
    "    params_dict['hydro_params_raw'] = torch.sigmoid(\n",
    "        params_hydro_model[:, :, :len(phy_model.parameters_bound) * config['dpl_model']['nmul']]).view(\n",
    "        params_hydro_model.shape[0], params_hydro_model.shape[1], len(phy_model.parameters_bound),\n",
    "        config['dpl_model']['nmul'])\n",
    "    \n",
    "    # Routing params\n",
    "    if config['routing_hydro_model'] == True:\n",
    "        params_dict['conv_params_hydro'] = torch.sigmoid(\n",
    "            params_hydro_model[-1, :, len(phy_model.parameters_bound) * config['dpl_model']['nmul']:])\n",
    "    else:\n",
    "        params_dict['conv_params_hydro'] = None\n",
    "    return params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the differentiable model object and a basic forward method.\n",
    "class dHBV(torch.nn.Module):\n",
    "    def __init__(self, phy_model, pnn_model):\n",
    "        super(dHBV, self).__init__()\n",
    "        self.pnn_model = pnn_model\n",
    "        self.phy_model = phy_model\n",
    "\n",
    "    def forward(self, config, x_dict):\n",
    "        \"\"\"Basic Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_dict : dict\n",
    "            Dictionary containing the input data for the model.\n",
    "        \"\"\"\n",
    "        # Forward pNN model;\n",
    "        # Take array of predicted params from pNN and unpack into dictionary.\n",
    "        params_all = self.pnn_model(x_dict['inputs_nn_scaled'])\n",
    "        params_dict = breakdown_params(config, self.phy_model, params_all)\n",
    "        \n",
    "        # Forward physics model\n",
    "        flow_out = self.phy_model(\n",
    "            x_dict['x_hydro_model'],\n",
    "            params_dict['hydro_params_raw'],\n",
    "            config,\n",
    "            static_idx=config['phy_model']['stat_param_idx'],\n",
    "            warm_up=config['phy_model']['warm_up'],\n",
    "            routing=config['routing_hydro_model'],\n",
    "            conv_params_hydro=params_dict['conv_params_hydro']\n",
    "        )\n",
    "\n",
    "        # Baseflow index percentage;\n",
    "        # Using two deep groundwater buckets: gwflow & bas_shallow\n",
    "        if 'bas_shallow' in flow_out.keys():\n",
    "            baseflow = flow_out['gwflow'] + flow_out['bas_shallow']\n",
    "        else:\n",
    "            baseflow = flow_out['gwflow']\n",
    "        flow_out['BFI_sim'] = 100 * (torch.sum(baseflow, dim=0) / (\n",
    "                torch.sum(flow_out['flow_sim'], dim=0) + 0.00001))[:, 0]\n",
    "\n",
    "        return flow_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize the differentiable model object, and assign to a GPU.\n",
    "dpl_model = dHBV(phy_model, pnn_model).to(config['device'])\n",
    "print(dpl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Init Optimizer\n",
    "\n",
    "We use the Adadelta optimizer from PyTorch, feeding it both learnable model\n",
    "parameters and a learning rate from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(\n",
    "    dpl_model.parameters(),\n",
    "    lr=config['pnn_model']['learning_rate']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Init Loss Function\n",
    "\n",
    "Dynamically load the loss function identified in the config (RMSE for\n",
    "dHBV 1.0 and NSE for dHBV 1.1p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.loss_functions import get_loss_fn\n",
    "\n",
    "loss_fn = get_loss_fn(config, dataset['obs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train the Model\n",
    "\n",
    "Below we show a basic training loop implementation that works for both \n",
    "dHBV 1.0 and dHBV 1.1p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from core.data import calc_training_params, take_sample_train\n",
    "from core.utils import save_model\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup training grid.\n",
    "n_grid, n_minibatch, nt = calc_training_params(\n",
    "    dataset['inputs_nn_scaled'],\n",
    "    config['train_t_range'],\n",
    "    config\n",
    "    )\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    total_loss = 0.0 # Initialize epoch loss to zero.\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "    \n",
    "    for i in tqdm.tqdm(range(1, n_minibatch + 1), desc=prog_str,\n",
    "                       leave=False, dynamic_ncols=True):\n",
    "        \n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_sample = take_sample_train(config, dataset, n_grid, nt)\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        predictions = dpl_model.forward(config, dataset_sample)        \n",
    "        \n",
    "        # Calculate loss.\n",
    "        loss = loss_fn(config,\n",
    "                       predictions,\n",
    "                       dataset_sample['obs'],\n",
    "                       igrid=dataset_sample['iGrid']\n",
    "                       )\n",
    "                                   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / n_minibatch + 1\n",
    "\n",
    "    log.info(f\"Avg model loss after epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    # Save the model every save_epoch (set in the confic).\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
