{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *DeltaModel* + *hydroDL2.0* Tutorial: **$\\delta$ HBV**\n",
    "Last Revision: 2 Dec. 2024\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this tutorial, weâ€™ll walk through a basic implementation of the generic differentiable modeling framework *DeltaModel*\n",
    "using the hydrology model [HBV](https://en.wikipedia.org/wiki/HBV_hydrology_model) from *hydroDL2.0* as a physics backbone.\n",
    "The resulting model we call $\\delta$ HBV (citation).\n",
    "    \n",
    "In general, the differentiable model modality presented here is composed of a coupled physics model and neural network.\n",
    "Physics models include parameters for which True values are often unknown, but approximated to varying degrees in practice.\n",
    "By coupling a neural network, we can learn a set of a physics model's parameters, which can then be fed back in\n",
    "alongside other known input variables to make predictions.\n",
    "\n",
    "In this tutorial, we use the physics model HBV and an LSTM parameterization network as the core of a differentiable model.\n",
    "Since HBV uses input forcing variables precipitation, temperature, and potential evapotranspiration (PET)\n",
    "with dimensions of days $x$ sites, the LSTM will learn parameters at every site for every day. The structure of the\n",
    "differentiable model, therefore, looks like so:\n",
    "\n",
    "$\n",
    "\\delta \\text{HBV} = \n",
    "\\begin{cases}\n",
    "\\text{Learned Parameters = } \\text{LSTM}(x \\text{, } A) \\\\\n",
    "\\text{Hydrologic Predictions = } \\text{HBV}(x \\text{, Learned Parameters})\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where\n",
    "- \\( $x$ \\) represents input weather forcings\n",
    "- \\( $A$ \\) is the set of basin attributes.\n",
    "\n",
    "    \n",
    "Currently, $\\delta$ HBV is setup to train and make hydrologic predictions on **streamflow**, but it can\n",
    "also be reconfigured without much effort to predict percolation, recharge, and groundwater flow, among others.\n",
    "\n",
    "\n",
    "After showing an example implementation, we'll demonstrate how to train the model and expose critical details of the process.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Setup\n",
    "- **Code**: Before beginning, please see `examples/differentiable hydrology/setup_guide.md`\n",
    "for steps on how to setup DeltaModel, hydroDL2, and the included Conda ENV (`envs/deltamod_env.yaml`)\n",
    "\n",
    "- **ENV**: For a quick start, we recommend using the `deltamod` ENV, which includes a minimal list of\n",
    "all core DeltaModel dependencies.\n",
    "\n",
    "- **Data**: Preconfigured CAMELS data files can be shared on request, but will eventually be made available through the MHPI\n",
    "data engine `hydro_data_dev` along with expanded instructions on how to access.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Creating a Model\n",
    "For this example, we demonstrate how to get DeltaModel running with minimal setup using the HBV implementation\n",
    "from hydroDL2. Note, there are two versions of $\\delta$ HBV which can be run here:\n",
    "\n",
    "- **$\\delta$ HBV 1.0**: Uses the standard version of the HBV model (citation).\n",
    "- **$\\delta$ HBV 1.1p**: An enhanced version of the HBV model, incorporating modifications like a capillary rise module to\n",
    "improve prediction performance over 1.0 (citation).\n",
    "\n",
    "Switching between these two versions requires only a simple configuration file change, as outlined below.\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### **Set Model and Experiment Configurations**\n",
    "    \n",
    "    Define your desired model settings and experimental parameters in a YAML configuration file. For this tutorial, two\n",
    "    configuration files have been premade for running both versions of $\\delta$ HBV. Each is configured to\n",
    "    use 531 CAMELS basins with 34 years of data available as inputs. For temporal tests, we have further configured\n",
    "    both files to run a 9-year training phase from 1999-2008 and 10-year test phase from 1989-1999 to reproduce benchmark\n",
    "    results of (citation):\n",
    "    -  **$\\delta$ HBV 1.0**: `example/conf/dhbv_config.yaml`\n",
    "    - **$\\delta$ HBV 1.1p**: `example/conf/dHBV_1_1p_config.yaml` \n",
    "\n",
    "- #### **Building the Model**\n",
    "\n",
    "    Currently, differentiable models can be built with DeltaModel in two ways of varying flexibility:\n",
    "    - **Implicit**: This method is best for small-scale experiments and distribution of final products.\n",
    "    \n",
    "        Add/change modules in DeltaModel to create your own differentiable model, and add to/change\n",
    "        configuration settings in `deltaModel/conf/config.yaml` to reflect the needs of your product. (Modules including\n",
    "        trainers, physics models, neural networks, loss functions, are designed to be hot-swappable per user needs. The\n",
    "        differentiable model modality will eventually also be made more flexible to meet diverse modeling needs. See\n",
    "        `docs/getting_started_guide` for more info on making the DeltaModel your own.) \n",
    "\n",
    "        With these pieces in place, simply run the model in your terminal using\n",
    "        ```shell\n",
    "        cd ./deltaModel\n",
    "        python __main__.py\n",
    "        ```\n",
    "\n",
    "    - **Explicit** (Code block below): This method is best for exploratory research and prototyping.\n",
    "\n",
    "        This approach is similar in that we still use a configuration file to hold model and experiment settings (though\n",
    "        a dictionary object can just as well be used). However, here we can expose the fundamental steps in the\n",
    "        modeling process; data preprocesing, model building, and experimentation/forwarding. In doing so, we make it\n",
    "        quicker to develop model and data pipelines, and easier to follow internal processes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Walkthrough\n",
    "The example below represents a simple, explicit implementation of a differentiable model using the HBV backbone:\n",
    "1.  **Load a configuration file**: Using Hydra and OmegaConf packages, we can convert our `..config.yaml` into a dictionary\n",
    "    object `config`. For example, if your config file contains `mode: train`, the dictionary yields\n",
    "    `config['mode'] == 'train'`. However, config can also contain sub-dictionaries. Take\n",
    "    ```yaml\n",
    "    training: \n",
    "        start_time: 1999/10/01\n",
    "    ```\n",
    "    for instance. In these cases, accessing the subdict can be done like `config['training']['start_time'] == '1999/10/01'`.\n",
    "\n",
    "2.  **Load in data**: At this step, we load and process our data as a dictionary of variable and attribute datasets\n",
    "    that are used by the neural network and physics model. In general, this dataset dict is supplied by the user, and should meet\n",
    "    minimum requirements established in `docs/getting_started_guide.md`.\n",
    "\n",
    "    For this example, we take a small, arbitrarily selected sample of the data to illustrate the modeling process.\n",
    "\n",
    "3.  **Initialize sub-models**: Next, we initialize the physics model and neural network our differentiable model\n",
    "    will use, HBV and LSTM in this case.\n",
    "\n",
    "4.  **Create a differentiable model**: Now, the sub-models are linked together by the DeltaModel wrapper. This\n",
    "    wrapper interfaces models using the desired modality, i.e., forwarding the LSTM to generate parameters, which are then fed with\n",
    "    forcing variables back into HBV to generate predictions. \n",
    "\n",
    "6.  **Forward/Experiment**: With the differentiable model created, it can be forwarded (as demonstrated\n",
    "    below), or trained/tested/applied in any user-defined experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamflow predictions for 365 days and 100 basins: Showing the first 5 days for 5 basins \n",
      " tensor([[[0.4509],\n",
      "         [0.5314],\n",
      "         [0.3484]],\n",
      "\n",
      "        [[0.4531],\n",
      "         [0.5746],\n",
      "         [0.3726]],\n",
      "\n",
      "        [[0.4521],\n",
      "         [0.6119],\n",
      "         [0.3941]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from hydroDL2.models.hbv.hbv import HBV as hbv\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "from deltaModel.core.data.data_loaders.hydro_loader import HydroDataLoader\n",
    "from deltaModel.core.data.data_samplers.hydro_sampler import take_sample\n",
    "\n",
    "\n",
    "CONFIG_PATH = '../example/conf/config_dHBV_1_1p.yaml'\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# 2. Setup a dataset dictionary of NN and physics model inputs.\n",
    "dataset = HydroDataLoader(config, test_split=True).eval_dataset\n",
    "dataset_sample = take_sample(config, dataset, days=730, basins=100)\n",
    "\n",
    "# 3. Initialize physical model and NN.\n",
    "phy_model = hbv(config['dpl_model']['phy_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# 4. Create the differentiable model dHBV: a torch.nn.Module that describes how \n",
    "# the NN is linked to the physical model HBV.\n",
    "dpl_model = dHBV(phy_model=phy_model, nn_model=nn)\n",
    "\n",
    "\n",
    "## From here, forward or train dpl_model just as any torch.nn.Module model.\n",
    "\n",
    "# 5. For example, to forward:\n",
    "output = dpl_model.forward(dataset_sample)\n",
    "\n",
    "\n",
    "print(f\"Streamflow predictions for {output['flow_sim'].shape[0]} days and {output['flow_sim'].shape[1]} basins: Showing the first 5 days for 5 basins \\n {output['flow_sim'][:3,:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Loading a Trained Model.\n",
    "\n",
    "What we just demonstrated was a naive model with no tuning. Now let's try loading\n",
    "dHBV/dHBV1.1p that has already been trained.\n",
    "\n",
    "*Will be added in coming update.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3. Training a Model\n",
    "\n",
    "Now that we can build a differentiable model, we will train $\\delta$ HBV (1.0 or 1.1p) and expose critical steps in the\n",
    "process, particularly within the model trainer.\n",
    "\n",
    "### 3.1 Load Config and Dataset\n",
    "\n",
    "Once again, we begin by loading in the config dict and CAMELS dataset.\n",
    "\n",
    "Be sure to change `CONFIG_PATH` to the HBV version you want to use.\n",
    "\n",
    "<!-- This code instantiates a model Trainer which will train or test a model per the user's specification in the config. Note\n",
    "that `__main__.py` is trimmed-down here to illustrate it's primary objective. The trainer performs the following\n",
    "functions:\n",
    "- CAMELS data will be loaded and preprocessed,\n",
    "- A differenial model object with the HBV1.1p backbone will be created, and \n",
    "- An optimizer and loss function will be initialized. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../deltaModel')  # Add the root directory of deltaModel\n",
    "\n",
    "from example import load_config \n",
    "from deltaModel.core.data.dataset_loading import get_dataset_dict # Eventually a hydroData import\n",
    "\n",
    "\n",
    "CONFIG_PATH = '../conf/dHBV1_1p_config.yaml'\n",
    "\n",
    "\n",
    "# Load configuration dictionary of model parameters and options\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Setup a dataset dictionary of NN and physics model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset = get_dataset_dict(config, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize a $\\delta$ HBV Differentiable Model, Optimizer, and Loss Function\n",
    "\n",
    "These are the auxillary tasks completed by the Trainer before beginning the training loop.\n",
    "\n",
    "\n",
    "<!-- We use the Adadelta optimizer from PyTorch, feeding it both learnable model\n",
    "parameters and a learning rate from the config file.\n",
    "\n",
    "\n",
    "Dynamically load the loss function identified in the config (RMSE for\n",
    "dHBV 1.0 and NSE for dHBV 1.1p). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from hydroDL2.models.hbv.hbv import HBVMulTDET as hbv\n",
    "from deltaModel.models.loss_functions import get_loss_fn\n",
    "from deltaModel.models.neural_networks import init_nn_model\n",
    "from deltaModel.models.differentiable_model import DeltaModel as dHBV\n",
    "\n",
    "\n",
    "\n",
    "# Initialize physical model and neural network\n",
    "phy_model = hbv(config['dpl_model'])\n",
    "nn = init_nn_model(phy_model, config['dpl_model'])\n",
    "\n",
    "# Create the differentiable model dHBV: \n",
    "# a torch.nn.Module that describes how nn is linked to the physical model.\n",
    "dpl_model = dHBV(phy_model, nn)\n",
    "print(f\"Here is our dHBV framework: \\n ----- \\n {dpl_model}\")\n",
    "\n",
    "# Init an Adadelta optimizer\n",
    "optimizer = torch.optim.Adadelta(\n",
    "    dpl_model.parameters(),\n",
    "    lr=config['dpl_model']['nn_model']['learning_rate']\n",
    "    )\n",
    "\n",
    "# init a loss function\n",
    "loss_fn = get_loss_fn(config, dataset['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train the Model\n",
    "\n",
    "Below we use a basic training loop to train the LSTM in $\\delta$ HBV to optimize HBV's parameters and improve its\n",
    "streamflow predictions.\n",
    "\n",
    "\n",
    "#### Key Steps in the Training Loop\n",
    "1. **Calculate Training Parameters**  \n",
    "   The `calc_training_params` function calculates the training settings:\n",
    "   - `n_sites`: The number of unique locations/sites in the dataset.\n",
    "   - `n_minibatch`: The number of samples to process per epoch.\n",
    "   - `n_timesteps`: The number of timesteps per sample.\n",
    "\n",
    "2. **Epoch Loop**  \n",
    "   Each epoch represents one full cycle through the training data. For each epoch:\n",
    "   - `total_loss` is reset to track the total error across all batches within the epoch.\n",
    "\n",
    "3. **Batch Loop**  \n",
    "   Within each epoch, the code processes data in smaller chunks (minibatches) to improve training efficiency and avoid\n",
    "   oversaturation of GPU VRAM. \n",
    "   \n",
    "   For each batch:\n",
    "   - **Sample Data**: `get_training_sample` randomly selects a sample of training data for the batch.\n",
    "   - **Forward Pass**: The model processes the input data to produce predictions.\n",
    "   - **Calculate Loss**: `loss_fn` compares predictions to observed values to calculate the error for the batch.\n",
    "   - **Backward Pass and Optimization**: \n",
    "     - `loss.backward()` computes gradients to adjust the modelâ€™s parameters.\n",
    "     - `optimizer.step()` updates the LSTM parameters.\n",
    "     - `optimizer.zero_grad()` resets gradients for the next batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from deltaModel.core.data import create_training_grid, get_training_sample\n",
    "from deltaModel.core.utils import save_model\n",
    "\n",
    "\n",
    "\n",
    "# Number of training samples per epoch, batch size, and number of timesteps\n",
    "n_samples, n_minibatch, n_timesteps = create_training_grid(\n",
    "    dataset['xc_nn_norm'],\n",
    "    config\n",
    ")\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    total_loss = 0.0 # Initialize epoch loss to zero.\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "    \n",
    "    for i in tqdm.tqdm(range(1, n_minibatch + 1), desc=prog_str,\n",
    "                       leave=False, dynamic_ncols=True):\n",
    "        \n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_sample = get_training_sample(config, dataset, n_samples, n_timesteps)\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        predictions = dpl_model.forward(dataset_sample)        \n",
    "        \n",
    "        # Calculate loss.\n",
    "        loss = loss_fn(\n",
    "            predictions,\n",
    "            dataset_sample['target'],\n",
    "            n_samples=dataset_sample['batch_sample']\n",
    "                       )\n",
    "                                   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / n_minibatch + 1\n",
    "    print(f\"Avg model loss after epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    # Save the model every save_epoch (set in the config).\n",
    "    model_name = config['dpl_model']['phy_model']['model_name']\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4. Test a Model\n",
    "\n",
    "If you have run testing on a trained model and want to view the results, you can find a `mstd.csv` file in your\n",
    "`results/` directory, which will present performance statistics for the respective model. \n",
    "\n",
    "*A testing tutorial and Graphical visualizations of model output will be supported in coming update.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
